{
  "hash": "8c1fc4e40353391fd2569292ff045317",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Scraping online press articles in R\"\nauthor: 'Lorenzo Mattioli'\ncategories:\n  - blog\ndate: 21 04 2025\nformat:\n  html:\n    toc: true\nlang: en_UK\ncode-tools: \n      source: https://github.com/lorMatt/UmbriaPress-corpus.git\nimage: scraper.png\nexecute:\n  eval: false\n  echo: true\n---\n\nWhen I had to choose the topic and methods for my Master's thesis, I decided it would be a great idea to use it as an excuse to start experimenting with natural language processing. Focusing on the Umbria region, where I grew up, I decided to embark on the completely new task of scraping and analysing local press articles and YouTube videos using the tools of computational social science. This is the first of a series of blog posts on the process of doing just that in R, from a beginner's perspective.\n\n------------------------------------------------------------------------\n\nIn this article, we will be using the local newspaper *Terninrete* as an example. You can find the entire corpus I built with these methods at this [link](https://lormatt.github.io/posts/UmbriaPress%20corpus/), and the complete code for all the scrapers through the `</> Code` button at the top of this page.\n\n## *Parsing* or *scraping*?\n\nWe refer to web *parsing* when we extract organised and formatted data, translating it into a format that we (and our machine) understand. In the context of textual analysis, this generally consists of downloading and reading xml files. What we're going to focus on here is web *scraping*, which consists of directly *attacking* a website's html and turning it into a viable format for data analysis.\n\nThe packages you'll need for his project are: `rvest`, `future`, `tidyverse`, `progressr`, `furrr`.\n\n::: callout-important\nI will **not** discuss the ethics of web scraping in this article. If you are just starting out, I really recommend [this short article](https://medium.com/data-science/ethics-in-web-scraping-b96b18136f01) on the topic for some quick and easy tips on how to be an ethical scraper and site owner.\n:::\n\n::: {.callout-warning collapse=\"true\"}\n# What I assume you already know\n\nYou'll need to already be well versed in R to understand the content of this blog post. If you're not familiar with R programming, I really recommend the [R for Data Science](https://r4ds.hadley.nz) manual. If you want some more step-by-step instructions on how to go about computational social science stuff, I found Felix Lennert's [tutorials](https://bookdown.org/f_lennert/toolbox-quarto-book/) to be very helpful.\n:::\n\n# Workflow\n\nWe can break the process of scraping a website into a few simple steps:\n\n-   Identifying the information we want to extract from a target website\n\n-   Find a way to isolate the information in the website's source code\n\n-   Write a script which stores it in a cozy, spreadsheet-like format\n\n-   Optimise it to reduce runtimes\n\nThe next few paragraphs will explain how to do all that in R, using the packages [`rvest`](https://rvest.tidyverse.org) and [`future`](https://future.futureverse.org).\n\n# Identifying the target information\n\nUnsurprisingly, the first thing one should do is to simply look at the webpage we're going to scrape and decide what data we're interested in scraping from it. In this example, *Terninrete*'s posts page currently looks like this:![](Images/Terninrete page.png)\n\nNow, we're interested in a specific set of variables for each article, which we'll need to target in the website's source code: the title, the text, and the date of each one. To do that, we'll actually need to teach our scraper to open each individual article page and scrape additional information (the actual text of each article) from there.\n\n# Isolating information in the website's source code\n\nThe best way to isolate parts of a web page is finding the html nodes connected to that specific title or link, as in css classes and the specific html tags containing them. The easiest way I found to do that is using the Chrome extension [selectorgadget](https://selectorgadget.com/), which does exactly that with a very easy visual interface. In our case, we'll first isolate the titles in the posts page to extract a list of URLs to each article. Doing that in Chrome looks something like this:![Green sections are selected, red are excluded, yellow are included](Images/Selectorgadget.png)\n\nWe can then simply copy-paste the snippet in the bottom panel to retrieve exclusively the data we need.\n\n# *Actual* scraping\n\nIn this examples, I actually built two scrapers to be used one after the other. One to retrieve the links to each article, the other to retrieve all article data (what we're actually interested in). We'll go though the first one step by step, since it is slightly easier to understand, but the workflow is exactly the same.\n\n## Define the URLs\n\nFirst thing to do is define a vector of links to be scraped. In this specific case, the `links`vector is simply a link of each one of the 2984 pages of articles posted by *Terninrete*:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinks <- paste0('https://terninrete.it/tutti-gli-articoli/page/', c(1:2984), '/')\n```\n:::\n\n\n## Retrieve data\n\nThis passage is at the core of the scraping process. Given an URL, we use `rvest::read_html` to load a page HTML, then we define a variable containing the information we need (in this case all the articles' `url`s) and we use the functions `rvest::html_elements` and `rvest::html_attr` to tell R where to find the information. In this case, we copy-paste the node we found with selectorgadget as a string inside `html_elements`, and then tell `rvest` to scrape the links at that location by targeting the appropriate html attribute (*href*).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml <- read_html(link) # load page HTML\n    \nurl <- html |> \n  html_elements('.jeg_post_title a') |> # scrape article URLs\n  html_attr('href')\n```\n:::\n\n\n## Automating the process\n\nNobody has time to run the scraper manually 2984 times, so we have a few options to automate the process. The easiest and most basic one is a *for* loop. we simply iterate the retrieval process for every item in our `links` vector:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTerninrete <- tibble( # setup tibble\n    url = character()\n    )\n\nfor (i in 1:NROW(links)) {\n  print(paste('scraping article number', i, 'of', NROW(article_urls)))\n  \n  html <- read_html(links[i]) # load page HTML\n    \n  url <- html |> \n    html_elements('.jeg_post_title a') |> # scrape article URLs\n    html_attr('href')\n  \n  n_row <- tibble(\n    url = url\n    )\n  \n  Terninrete <- n_row |> \n    bind_rows(Terninrete) # append to original df\n  \n}\n```\n:::\n\n\nAnother option is defining a scraping function and then *mapping* it to the same vector, taking advantage of `purr::map()`. This is, in my opinion, a much better option: it lets us take care of errors when they arise, and potentially optimise the process to reduce runtimes, sparing us a *lot* of headaches and execution halts.\n\n### Defining a scraping function\n\nIt is always a good idea to wrap one's functions inside a `tryCatch()` command, to instruct R on what to do when the function fails. In this case, we tell it to return a tibble row with the URLs if the function works properly, and a `NA_character_` if an error arises.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl_scrape <- function(link) {\n  tryCatch({\n    \n    html <- read_html(link) # Load page HTML\n    \n    url <- html |> \n      html_elements('.jeg_post_title a') |> # scrape article URLs\n      html_attr('href')\n    \n    return(tibble(url = url))\n    \n  }, error = function(e) {\n    \n    return(tibble(url = NA_character_, errore = as.character(e)))\n    \n  })\n}\n```\n:::\n\n\n### Mapping it to an URL vector\n\nThe scraper is basically done, we can then easily map it to the links vector:\n\n\n::: {.cell}\n\n```{.r .cell-code}\narticle_links <- map(links, url_scrape(link))\n```\n:::\n\n\n# Using parallel sessions to reduce runtimes\n\nWhen working with long lists of URLs, a simple loop or a simple function mapping script might take hours or even days to complete. A good option to reduce runtimes is configuring parallel sessions running the same function. The `future` environment offers a perfectly streamlined interface to do so. The example below also implements the package `progressr` to set up a progress bar, very useful in keeping track of the process.\n\nThe first step is to configure parallel sessions with `future::plan()`. This function can set up multi-core or multi-session runs of a function. In this case, we configure a six-session run for the function that we defined above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplan(multisession, workers = 6)\n```\n:::\n\n\nWe then set up prgressr by initialising the progress bar and nest our `url_scrape()` function inside a dummy function which also increments the progress bar. To map our new function to the links vector, we use the *futureverse* version of `purrr`, `furrr`. The new command we'll use is `furrr::future_map()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith_progress({\n  p <- progressor(steps = length(links)) # initialise prog bar\n  \n  # Parallel scraping\n  article_links <- future_map(links, function(link) {\n    p() # Progress bar increment\n    url_scrape(link)\n  })\n})\n```\n:::\n\n\nThe final step is as easy as cleaning up the results. Being a tidyverse fan, I did so with the following pipeline:\n\n\n::: {.cell}\n\n```{.r .cell-code}\narticle_links <- bind_rows(article_links) |>\n  unique() |> \n  filter(is.na(errore)) |> \n  select(url)\narticle_links <- article_links$url |>\n  as.character()\n```\n:::\n\n\nIn this specific example, all that was left to do was repeat the same steps on the new list of links to each article, and wait for the computer to compile a handy tibble with all the information we need.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n## define scraping function ----\nart_scrape <- function(link) {\n  tryCatch({\n    html <- read_html(link)\n    \n    title <- html |> \n      html_elements('.jeg_post_subtitle , .entry-header .jeg_post_title') |> \n      html_text() |> \n      paste(collapse = '. ')\n    \n    date <- html |> \n      html_element('.meta_left .jeg_meta_date a') |> \n      html_text() |> \n      str_extract('\\\\d{2}\\\\s[A-Za-z]+\\\\s\\\\d{4}')\n    \n    text <- html |> \n      html_elements('.content-inner p') |> \n      html_text() |> \n      paste(collapse = ' ')\n    \n    return(tibble(title = title,\n                  date = date,\n                  text = text))\n  }, error = function(e) {\n    return(tibble(title = NA,\n                  date = NA,\n                  text = NA,\n                  errore = as.character(e)))\n  })\n}\n\n# configure future for parallel run\nplan(multisession, workers = 8)\nart_links <- article_links\n## progressr config, call function ----\nwith_progress({\n  p <- progressor(steps = length(art_links)) # Inizializza la barra di progresso\n  \n  # Parallel scraping\n  article_data <- future_map(art_links, function(link) { # Usa future_map qui\n    p() # Progress bar increment\n    art_scrape(link)\n  })\n})\n\n# convert results in tibble, parse dates\n## set language to italian\nSys.setlocale(locale=\"it_IT.UTF-8\")\nTRinrete <- tibble(title = character(),\n                   date = Date(),\n                   text = character(),\n                   errore = character())\nTRinrete_ <- bind_rows(article_data) |> \n  mutate(date = date |> as_date(format = \"%d %B %Y\")) |> \n  filter(is.na(errore)) |> \n  bind_rows(TRinrete) |> \n  unique()\nTRinrete <- TRinrete_ |> rbind(TRinrete)\n```\n:::\n\n\n# What's in it for us?\n\nAfter all this hard work, what we're left with is a machine-readable, easy-to-navigate tibble of free data to do some exciting analysis on. Once again, the data I scraped is readily available at this [link](https://lormatt.github.io/posts/UmbriaPress%20corpus/), and the source codes (which very much reflect my learning curve) can be found through the `</> Code` button at the top of this page.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}